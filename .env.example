# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                         QUORUM CONFIGURATION                             ║
# ╚══════════════════════════════════════════════════════════════════════════╝
#
# Copy this file to .env and fill in your API keys and preferred models.
#
# IMPORTANT: Cloud provider models (OpenAI, Anthropic, Google, xAI) must be
# listed in their respective *_MODELS variables to be available in Quorum.
# Ollama models are auto-discovered and don't need to be listed.


# ══════════════════════════════════════════════════════════════════════════════
# OPENAI
# ══════════════════════════════════════════════════════════════════════════════
# Get your API key: https://platform.openai.com/api-keys
# Available models: https://platform.openai.com/docs/models
#
OPENAI_API_KEY=sk-...
OPENAI_MODELS=gpt-5.2,gpt-5.1,gpt-5


# ══════════════════════════════════════════════════════════════════════════════
# ANTHROPIC
# ══════════════════════════════════════════════════════════════════════════════
# Get your API key: https://console.anthropic.com/settings/keys
# Available models: https://docs.anthropic.com/en/docs/about-claude/models
#
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODELS=claude-opus-4-5-20251124,claude-sonnet-4-5-20250929


# ══════════════════════════════════════════════════════════════════════════════
# GOOGLE
# ══════════════════════════════════════════════════════════════════════════════
# Get your API key: https://aistudio.google.com/apikey
# Available models: https://ai.google.dev/gemini-api/docs/models
#
GOOGLE_API_KEY=...
GOOGLE_MODELS=gemini-3-pro,gemini-2.5-flash


# ══════════════════════════════════════════════════════════════════════════════
# XAI (GROK)
# ══════════════════════════════════════════════════════════════════════════════
# Get your API key: https://console.x.ai/
# Available models: https://docs.x.ai/docs/models
#
XAI_API_KEY=xai-...
XAI_MODELS=grok-4.1,grok-4


# ══════════════════════════════════════════════════════════════════════════════
# OLLAMA (LOCAL MODELS)
# ══════════════════════════════════════════════════════════════════════════════
# Install Ollama: https://ollama.com/download
# Available models: https://ollama.com/library
#
# Models are auto-discovered - no configuration needed for same-machine setup!
# Just run "ollama pull llama3" and it appears in /models automatically.
#
# Only configure these if Ollama runs on a different machine:
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_API_KEY=                          # Optional, for proxy authentication
#
# WSL users with Ollama on Windows: See README.md troubleshooting section


# ══════════════════════════════════════════════════════════════════════════════
# QUORUM SETTINGS
# ══════════════════════════════════════════════════════════════════════════════

# Language setting
# Leave empty/unset to respond in the same language as the question
# Or set a specific language to always use
# QUORUM_DEFAULT_LANGUAGE=Swedish

# ──────────────────────────────────────────────────────────────────────────────
# Discussion Methods
# ──────────────────────────────────────────────────────────────────────────────
# Choose your method in the terminal UI. Each has authentic structure:
#
# STANDARD (2+ models)   - Configurable consensus-seeking discussion
# OXFORD (even number)   - Formal parliamentary debate with FOR/AGAINST teams
# ADVOCATE (3+ models)   - Devil's advocate challenges the consensus
# SOCRATIC (2+ models)   - Question-driven Socratic dialogue
# DELPHI (3+ models)     - Iterative consensus for estimates and forecasts
# BRAINSTORM (2+ models) - Creative ideation (diverge → build → converge)
# TRADEOFF (2+ models)   - Structured comparison of alternatives
#
# NOTE: Only Standard method uses the settings below.
# Other methods have fixed authentic structures.

# ──────────────────────────────────────────────────────────────────────────────
# Standard Method Settings
# ──────────────────────────────────────────────────────────────────────────────

# Rounds per agent in Standard's Phase 3 discussion (default: 2)
# Total discussion messages = rounds_per_agent * number_of_models
QUORUM_ROUNDS_PER_AGENT=2

# Which model creates the final synthesis in Standard's Phase 5
# first  - Always use the first selected model (default, predictable)
# random - Randomly select a model each time
# rotate - Rotate through models across discussions
QUORUM_SYNTHESIZER=first

# ──────────────────────────────────────────────────────────────────────────────
# Model Timeout
# ──────────────────────────────────────────────────────────────────────────────
# Maximum time to wait for a model response (in seconds).
# Default: 60 seconds
# Increase for slow local models (e.g., Ollama on CPU): 120-300
#
# QUORUM_MODEL_TIMEOUT=60

# ──────────────────────────────────────────────────────────────────────────────
# Auto-save Reports
# ──────────────────────────────────────────────────────────────────────────────
# All discussions are automatically saved as markdown files.
# Configure the directory here (supports ~ for home directory expansion).
# Default: ~/reports
#
# QUORUM_REPORT_DIR=~/reports

# ──────────────────────────────────────────────────────────────────────────────
# Export Settings
# ──────────────────────────────────────────────────────────────────────────────
# Directory for exported discussions (manual /export command).
# Supports ~ for home directory expansion.
# If not set, exports to home directory (~).
#
# QUORUM_EXPORT_DIR=~/.quorum/exports

# Default export format: md (markdown), text (plain text), pdf, json
# Can be overridden per export: /export text, /export pdf, /export json
#
# QUORUM_EXPORT_FORMAT=md
